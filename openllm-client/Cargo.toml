[package]
name = "openllm-client"
version = "0.2.26"
edition = "2021"
description = "OpenLLM Client: Interacting with any OpenLLM HTTP/gRPC Service (BentoML included)"
authors = ["Aaron Pham <aarnphm@bentoml.com>", "BentoML Team <contact@bentoml.com>"]
readme = "README.md"
homepage = "https://bentoml.com"
repository = "https://github.com/bentoml/OpenLLM"
license = "Apache-2.0"
categories = ["web-programming::http-client", "model-serving", "mlops", "ai"]
keywords = [
  "MLOps",
  "AI",
  "BentoML",
  "Model Serving",
  "Model Deployment",
  "LLMOps",
  "Falcon",
  "Vicuna",
  "Llama 2",
  "Fine tuning",
  "Serverless",
  "Large Language Model",
  "Generative AI",
  "StableLM",
  "Alpaca",
  "PyTorch",
  "Transformers",
]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
[lib]
name = "openllm_client"
crate-type = ["cdylib"]

[dependencies]
hyper = "0.14.27"
prost = "0.11.9"
prost-types = "0.11.9"
pyo3 = "0.19.2"
reqwest = { version = "0.11.18", features = ["json"] }
serde = { version = "1.0.183", features = ["derive"] }
serde_json = "1.0.105"
tokio = { version = "1.32.0", features = ["macros", "rt-multi-thread", "net", "time"] }
tonic = { version = "0.9.2" , features = ["transport", "stream"] }

[build-dependencies]
prost-build = "0.11.9"
tonic-build = "0.9.2"
